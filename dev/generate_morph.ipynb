{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate morphology JSON files from CATSS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import regex \n",
    "import collections\n",
    "from greekutils import beta2unicode\n",
    "from pathlib import Path\n",
    "data = Path('../source/patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map book names (some books are split up); filenames copied/pasted from above printout\n",
    "book_norms = {    \n",
    "    '01.Gen.1.mlxx':'01.GEN.mlxx',\n",
    "    '02.Gen.2.mlxx':'01.GEN.mlxx',\n",
    "    '03.Exod.mlxx':'02.EXO.mlxx',\n",
    "    '04.Lev.mlxx':'03.LEV.mlxx',\n",
    "    '05.Num.mlxx':'04.NUM.mlxx',\n",
    "    '06.Deut.mlxx':'05.DEU.mlxx',\n",
    "    '07.JoshB.mlxx':'06.JOS_B.mlxx',\n",
    "    '08.JoshA.mlxx':'07.JOS_A.mlxx',\n",
    "    '09.JudgesB.mlxx':'08.JDG_B.mlxx',\n",
    "    '10.JudgesA.mlxx':'09.JDG_A.mlxx',\n",
    "    '11.Ruth.mlxx':'10.RUT.mlxx',\n",
    "    '12.1Sam.mlxx':'11.1SA.mlxx',\n",
    "    '13.2Sam.mlxx':'12.2SA.mlxx',\n",
    "    '14.1Kings.mlxx':'13.1KI.mlxx',\n",
    "    '15.2Kings.mlxx':'14.2KI.mlxx',\n",
    "    '16.1Chron.mlxx':'15.1CH.mlxx',\n",
    "    '17.2Chron.mlxx':'16.2CH.mlxx',\n",
    "    '18.1Esdras.mlxx':'17.1ES.mlxx',\n",
    "    '19.2Esdras.mlxx':'18.2ES.mlxx',\n",
    "    '20.Esther.mlxx':'19.ESG.mlxx',\n",
    "    '21.Judith.mlxx':'20.JDT.mlxx',\n",
    "    '22.TobitBA.mlxx':'21.TOB_BA.mlxx',\n",
    "    '23.TobitS.mlxx':'22.TOB_S.mlxx',\n",
    "    '24.1Macc.mlxx':'23.1MA.mlxx',\n",
    "    '25.2Macc.mlxx':'24.2MA.mlxx',\n",
    "    '26.3Macc.mlxx':'25.3MA.mlxx',\n",
    "    '27.4Macc.mlxx':'26.4MA.mlxx',\n",
    "    '28.Psalms1.mlxx':'27.PSA.mlxx',\n",
    "    '29.Psalms2.mlxx':'27.PSA.mlxx',\n",
    "    '30.Odes.mlxx':'28.ODA.mlxx',\n",
    "    '31.Proverbs.mlxx':'29.PRO.mlxx',\n",
    "    '32.Qoheleth.mlxx':'30.ECC.mlxx',\n",
    "    '33.Canticles.mlxx':'31.SNG.mlxx',\n",
    "    '34.Job.mlxx':'32.JOB.mlxx',\n",
    "    '35.Wisdom.mlxx':'33.WIS.mlxx',\n",
    "    '36.Sirach.mlxx':'34.SIR.mlxx',\n",
    "    '37.PsSol.mlxx':'35.PSS.mlxx',\n",
    "    '38.Hosea.mlxx':'36.HOS.mlxx',\n",
    "    '39.Micah.mlxx':'37.MIC.mlxx',\n",
    "    '40.Amos.mlxx':'38.AMO.mlxx',\n",
    "    '41.Joel.mlxx':'39.JOL.mlxx',\n",
    "    '42.Jonah.mlxx':'40.JON.mlxx',\n",
    "    '43.Obadiah.mlxx':'41.OBA.mlxx',\n",
    "    '44.Nahum.mlxx':'42.NAM.mlxx',\n",
    "    '45.Habakkuk.mlxx':'43.HAB.mlxx',\n",
    "    '46.Zeph.mlxx':'44.ZEP.mlxx',\n",
    "    '47.Haggai.mlxx':'45.HAG.mlxx',\n",
    "    '48.Zech.mlxx':'46.ZEC.mlxx',\n",
    "    '49.Malachi.mlxx':'47.MAL.mlxx',\n",
    "    '50.Isaiah1.mlxx':'48.ISA.mlxx',\n",
    "    '51.Isaiah2.mlxx':'48.ISA.mlxx',\n",
    "    '52.Jer1.mlxx':'49.JER.mlxx',\n",
    "    '53.Jer2.mlxx':'49.JER.mlxx',\n",
    "    '54.Baruch.mlxx':'50.BAR.mlxx',\n",
    "    '55.EpJer.mlxx':'51.LJE.mlxx',\n",
    "    '56.Lam.mlxx':'52.LAM.mlxx',\n",
    "    '57.Ezek1.mlxx':'53.EZE.mlxx',\n",
    "    '58.Ezek2.mlxx':'53.EZE.mlxx',\n",
    "    '59.BelOG.mlxx':'54.BEL_OG.mlxx',\n",
    "    '60.BelTh.mlxx':'55.BEL_TH.mlxx',\n",
    "    '61.DanielOG.mlxx':'56.DAG.mlxx',\n",
    "    '62.DanielTh.mlxx':'57.DAG_TH.mlxx',\n",
    "    '63.SusOG.mlxx':'58.SUS_OG.mlxx',\n",
    "    '64.SusTh.mlxx':'59.SUS_TH.mlxx' \n",
    "}\n",
    "\n",
    "\n",
    "final_letter = r'{}(?=\\s|$)'\n",
    "\n",
    "final_grk = [\n",
    "    (regex.compile(final_letter.format('σ')), 'ς'),\n",
    "]\n",
    "\n",
    "def sub_final(string, re_set):\n",
    "    \"\"\"Substitute final letters in Hebrew\"\"\"\n",
    "    for patt, repl in re_set:\n",
    "        string = patt.sub(repl, string)\n",
    "    return string\n",
    "\n",
    "def utf8_greek(string):\n",
    "    \"\"\"Convert transcribed Greek to UTF8\"\"\"\n",
    "    utf8_string = beta2unicode.convert(string)\n",
    "    finalized_string = sub_final(utf8_string, final_grk)\n",
    "    return finalized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing words for 01.Gen.1.mlxx...\n",
      "processing words for 02.Gen.2.mlxx...\n",
      "processing words for 03.Exod.mlxx...\n",
      "processing words for 04.Lev.mlxx...\n",
      "processing words for 05.Num.mlxx...\n",
      "processing words for 06.Deut.mlxx...\n",
      "processing words for 07.JoshB.mlxx...\n",
      "processing words for 08.JoshA.mlxx...\n",
      "processing words for 09.JudgesB.mlxx...\n",
      "processing words for 10.JudgesA.mlxx...\n",
      "processing words for 11.Ruth.mlxx...\n",
      "processing words for 12.1Sam.mlxx...\n",
      "processing words for 13.2Sam.mlxx...\n",
      "processing words for 14.1Kings.mlxx...\n",
      "processing words for 15.2Kings.mlxx...\n",
      "processing words for 16.1Chron.mlxx...\n",
      "processing words for 17.2Chron.mlxx...\n",
      "processing words for 18.1Esdras.mlxx...\n",
      "processing words for 19.2Esdras.mlxx...\n",
      "processing words for 20.Esther.mlxx...\n",
      "processing words for 21.Judith.mlxx...\n",
      "processing words for 22.TobitBA.mlxx...\n",
      "processing words for 23.TobitS.mlxx...\n",
      "processing words for 24.1Macc.mlxx...\n",
      "processing words for 25.2Macc.mlxx...\n",
      "processing words for 26.3Macc.mlxx...\n",
      "processing words for 27.4Macc.mlxx...\n",
      "processing words for 28.Psalms1.mlxx...\n",
      "processing words for 29.Psalms2.mlxx...\n",
      "processing words for 30.Odes.mlxx...\n",
      "processing words for 31.Proverbs.mlxx...\n",
      "processing words for 32.Qoheleth.mlxx...\n",
      "processing words for 33.Canticles.mlxx...\n",
      "processing words for 34.Job.mlxx...\n",
      "processing words for 35.Wisdom.mlxx...\n",
      "processing words for 36.Sirach.mlxx...\n",
      "processing words for 37.PsSol.mlxx...\n",
      "processing words for 38.Hosea.mlxx...\n",
      "processing words for 39.Micah.mlxx...\n",
      "processing words for 40.Amos.mlxx...\n",
      "processing words for 41.Joel.mlxx...\n",
      "processing words for 42.Jonah.mlxx...\n",
      "processing words for 43.Obadiah.mlxx...\n",
      "processing words for 44.Nahum.mlxx...\n",
      "processing words for 45.Habakkuk.mlxx...\n",
      "processing words for 46.Zeph.mlxx...\n",
      "processing words for 47.Haggai.mlxx...\n",
      "processing words for 48.Zech.mlxx...\n",
      "processing words for 49.Malachi.mlxx...\n",
      "processing words for 50.Isaiah1.mlxx...\n",
      "processing words for 51.Isaiah2.mlxx...\n",
      "processing words for 52.Jer1.mlxx...\n",
      "processing words for 53.Jer2.mlxx...\n",
      "processing words for 54.Baruch.mlxx...\n",
      "processing words for 55.EpJer.mlxx...\n",
      "processing words for 56.Lam.mlxx...\n",
      "processing words for 57.Ezek1.mlxx...\n",
      "processing words for 58.Ezek2.mlxx...\n",
      "processing words for 59.BelOG.mlxx...\n",
      "processing words for 60.BelTh.mlxx...\n",
      "processing words for 61.DanielOG.mlxx...\n",
      "processing words for 62.DanielTh.mlxx...\n",
      "processing words for 63.SusOG.mlxx...\n",
      "processing words for 64.SusTh.mlxx...\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "morph_data = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    \n",
    "for file in sorted(data.glob('*.mlxx')):\n",
    "    \n",
    "    new_file = book_norms[file.name]\n",
    "    book_name = new_file.split('.')[1]\n",
    "    \n",
    "    lines = file.read_text().split('\\n')\n",
    "        \n",
    "    print(f'processing words for {file.name}...')\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "\n",
    "        line_data = line.strip().split()\n",
    "\n",
    "        # length of 0/1 is either blank line or section marker with no chapter/verse label\n",
    "        if len(line_data) == 1 and line_data[0] == '':\n",
    "            continue\n",
    "        # exception for some superscriptions or in-doubt texts w/out chapter:verse label\n",
    "        elif len(line_data) == 1 and line_data[0] != '': \n",
    "            line_data.append('0:0') # place-holder chapter:verse\n",
    "\n",
    "        if len(line_data) == 2:\n",
    "            ref_str = f'{book_name} {line_data[1]}'\n",
    "            \n",
    "        # length > 2 is a slot\n",
    "        elif len(line_data) > 2:\n",
    "    \n",
    "            # get slot data\n",
    "            trans = line_data[0]\n",
    "            morph = '.'.join(line_data[1:]) # morpho data into dot-separated string, disambiguate later\n",
    "            utf8 = utf8_greek(trans)\n",
    "            morph_data[new_file][ref_str].append((utf8, morph, trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ἐν', 'P.E)N', 'E)N'),\n",
       " ('ἀρχῇ', 'N1.DSF.A)RXH/', 'A)RXH=|'),\n",
       " ('ἐποίησεν', 'VAI.AAI3S.POIE/W', 'E)POI/HSEN'),\n",
       " ('ὁ', 'RA.NSM.O(', 'O('),\n",
       " ('θεὸς', 'N2.NSM.QEO/S', 'QEO\\\\S'),\n",
       " ('τὸν', 'RA.ASM.O(', 'TO\\\\N'),\n",
       " ('οὐρανὸν', 'N2.ASM.OU)RANO/S', 'OU)RANO\\\\N'),\n",
       " ('καὶ', 'C.KAI/', 'KAI\\\\'),\n",
       " ('τὴν', 'RA.ASF.O(', 'TH\\\\N'),\n",
       " ('γῆν', 'N1.ASF.GH=', 'GH=N')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_data['01.GEN.mlxx']['GEN 1:1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes to myself\n",
    "# prototypical counts per type:\n",
    "# 3 - adjv, noun, verb\n",
    "# 2 - advb, conj, intj, part, prep, inum\n",
    "# 3 - inum, pron, propn\n",
    "# 4 - propn (N.N.M.MESRAIM), verb (participle)\n",
    "\n",
    "# those with overloaded lexemes:\n",
    "# verb(>3, not participle), verb(>4, participle)\n",
    "\n",
    "# store new features here: feature_name to node to feature \n",
    "features = collections.defaultdict(dict)\n",
    "\n",
    "# conversion dicts\n",
    "typs = {'N': 'noun',\n",
    "        'V': 'verb',\n",
    "        'A': 'adjv',\n",
    "        'R': 'pron',\n",
    "        'C': 'conj',\n",
    "        'X': 'part',\n",
    "        'I': 'intj',\n",
    "        'M': 'inum',\n",
    "        'P': 'prep',\n",
    "        'D': 'advb'}\n",
    "       #'N': 'propn' proper noun, added below with special rule\n",
    "    \n",
    "# nominals \n",
    "# [case][number][gender]\n",
    "cases = {'N': 'nom',\n",
    "         'G': 'gen',\n",
    "         'D': 'dat',\n",
    "         'A': 'acc',\n",
    "         'V': 'voc'}\n",
    "numbers = {'S': 'sg',\n",
    "          'D': 'du',\n",
    "          'P': 'pl'}\n",
    "genders = {'M': 'm',\n",
    "          'F': 'f',\n",
    "          'N': 'n'}\n",
    "degrees = {'C': 'comparative',\n",
    "          'S': 'superlative'}\n",
    "\n",
    "# verbs\n",
    "# [tense][voice][mood][person][number] [case][number][gender]\n",
    "\n",
    "tenses = {'P': 'present',\n",
    "         'I': 'imperfect',\n",
    "         'F': 'future',\n",
    "         'A': 'aorist',\n",
    "         'X': 'perfect',\n",
    "         'Y': 'pluperfect'}\n",
    "voices = {'A': 'active',\n",
    "         'M': 'middle',\n",
    "         'P': 'passsive'}\n",
    "moods = {'I': 'indc',\n",
    "         'D': 'impv',\n",
    "         'S': 'subj',\n",
    "         'O': 'optv',\n",
    "         'N': 'infv',\n",
    "         'P': 'ptcp'}\n",
    "    \n",
    "def parse_morpho(morph_str):\n",
    "    \"\"\"Parse dot-separated LXX morphology string\"\"\"\n",
    "    \n",
    "    split_morph = morph_str.split('.')\n",
    "\n",
    "    morph_code = '.'.join(split_morph[:-1])\n",
    "    \n",
    "    # parse morphology codes in order of appearance:\n",
    "\n",
    "    # 1. assign subtypes and types\n",
    "\n",
    "    styp = split_morph[0] # subtype\n",
    "\n",
    "    # get type; exception for proper nouns; nouns with no subtypes\n",
    "    if styp == 'N':\n",
    "        typ = 'propn'\n",
    "    else:\n",
    "        typ = typs[styp[0]] # type is only first char of code, convert it\n",
    "\n",
    "    # 2. assign parsing data\n",
    "\n",
    "    # indeclinable words\n",
    "    if len(split_morph) == 2 or typ in {'advb', 'conj'}:\n",
    "        case, gender, number, degree, tense, voice, mood, person = ('' for i in range(1,9))\n",
    "        lexeme = '.'.join(split_morph[1:])\n",
    "\n",
    "    # nominal words with case/gender/number\n",
    "    elif typ in {'adjv', 'noun', 'inum', 'pron', 'propn'}:\n",
    "\n",
    "        parsing_data = split_morph[1]\n",
    "        case = ''\n",
    "        gender = ''\n",
    "        number = ''\n",
    "        degree = ''\n",
    "\n",
    "        # get parsing; some parsing codes have < 3 values, loop is thus necessary\n",
    "        for i, char in enumerate(parsing_data):\n",
    "\n",
    "            # dative/dual disambiguation\n",
    "            if i == 0 and char == 'D': \n",
    "                case = 'dat'\n",
    "            elif i != 0 and char == 'D':\n",
    "                number = 'du'\n",
    "\n",
    "            # disambiguation for 'S' superlative\n",
    "            elif all([char == 'S' or char == 'C', len(parsing_data) == 4,\n",
    "                      typ == 'adjv', i != 1]):\n",
    "                degree = degrees.get(char, '')\n",
    "\n",
    "            # all other parsings\n",
    "            elif char != 'D':\n",
    "                case = cases.get(char, '') if not case else case\n",
    "                gender = genders.get(char, '') if not gender else gender\n",
    "                number = numbers.get(char, '') if not number else number\n",
    "                degree = '' if not degree else degree\n",
    "\n",
    "        # set non applicable values to null\n",
    "        person, tense, voice, mood = ('' for i in range(1,5))\n",
    "\n",
    "        lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "\n",
    "    # verbs\n",
    "    elif typ == 'verb':\n",
    "\n",
    "        parsing_data = split_morph[1]\n",
    "        tense = tenses[parsing_data[0] ]\n",
    "        try:\n",
    "            voice = voices[parsing_data[1]]\n",
    "\n",
    "        except:\n",
    "            raise Exception(morph_str)\n",
    "        mood = moods[parsing_data[2]]\n",
    "\n",
    "        # handle participles \n",
    "        try:\n",
    "            gender = genders[parsing_data[5]] # only participles have >4 chars\n",
    "            number = numbers[parsing_data[4]]\n",
    "            case = cases[parsing_data[3]]\n",
    "            person = '' # non-applicable values\n",
    "            degree = ''\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            # all normal verbs\n",
    "            try:\n",
    "                person = parsing_data[3]\n",
    "                number = numbers[parsing_data[4]]\n",
    "                case = '' # non-applicable values\n",
    "                gender = '' \n",
    "                degree = ''\n",
    "\n",
    "            # handle infinitives\n",
    "            except IndexError: \n",
    "                person = '' # non-applicable values\n",
    "                number = ''\n",
    "                case = ''\n",
    "                gender = ''\n",
    "                degree = ''\n",
    "\n",
    "        lexeme = '.'.join(split_morph[2:])\n",
    "\n",
    "    # return features\n",
    "    features = {\n",
    "        'typ': typ,\n",
    "        'styp': styp,\n",
    "        'lexeme': lexeme,\n",
    "        'morph_code': morph_code,\n",
    "        'case': case,\n",
    "        'number': number,\n",
    "        'gender': gender,\n",
    "        'degree': degree,\n",
    "        'tense': tense,\n",
    "        'voice': voice,\n",
    "        'mood': mood,\n",
    "        'person': person,\n",
    "    }\n",
    "    \n",
    "    # filter empty features\n",
    "    features = {k:v for k,v in features.items() if v}\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# reassemble data here into a list of lists\n",
    "morph_data_plus = []\n",
    "\n",
    "for book, verses in morph_data.items():\n",
    "    book_data = [book]\n",
    "    for verse_ref, lines in verses.items():\n",
    "        verse_data = [verse_ref]\n",
    "        for utf8, morpho, trans in lines:\n",
    "            word_data = {'utf8': utf8, 'trans':trans}\n",
    "            word_data.update(parse_morpho(morpho))\n",
    "            verse_data.append(word_data)\n",
    "        book_data.append(verse_data)\n",
    "    morph_data_plus.append(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(morph_data_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# export prototype dataset\n",
    "out_dir = Path('../JSON/morphology')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "for book_data in morph_data_plus:\n",
    "    file_name = out_dir.joinpath(Path(book_data[0] + '.json'))\n",
    "    file_data = book_data[1:]\n",
    "    with open(file_name, 'w', encoding='UTF8') as outfile:\n",
    "        json.dump(file_data, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
