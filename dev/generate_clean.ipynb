{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Parallel Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean dataset is a modernization of the CATSS Database, presented with minimal changes, in UTF8, exported as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import regex \n",
    "import collections\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from greekutils import beta2unicode # do: pip install greek-utils==0.2\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append('../')\n",
    "import regex_patterns as repatts\n",
    "\n",
    "data = Path('../source/patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCAT transcription to UTF8\n",
    "# Greek to be handled by greekutils\n",
    "\n",
    "# Hebrew\n",
    "trans2utf8 = {\n",
    "    ')': 'א',\n",
    "    'B': 'ב',\n",
    "    'G': 'ג',\n",
    "    'D': 'ד',\n",
    "    'H': 'ה',\n",
    "    'W': 'ו',\n",
    "    'Z': 'ז',\n",
    "    'X': 'ח',\n",
    "    '+': 'ט',\n",
    "    'Y': 'י',\n",
    "    'K': 'כ',\n",
    "    'L': 'ל',\n",
    "    'M': 'מ',\n",
    "    'N': 'נ',\n",
    "    'S': 'ס',\n",
    "    '(': 'ע',\n",
    "    'P': 'פ',\n",
    "    'C': 'צ',\n",
    "    'Q': 'ק',\n",
    "    'R': 'ר',\n",
    "    '&': 'שׂ',\n",
    "    '$': 'שׁ',\n",
    "    'T': 'ת',\n",
    "    '-': '־',\n",
    "    '\\\\': '',\n",
    "    ' ': ' ',\n",
    "}\n",
    "\n",
    "hfletter = r'{}(?=\\s|$)'\n",
    "final_heb = (\n",
    "    (hfletter.format('\\u05DE'), 'ם'),\n",
    "    (hfletter.format('\\u05DB'), 'ך'),\n",
    "    (hfletter.format('\\u05E0'), 'ן'),\n",
    "    (hfletter.format('\\u05E4'), 'ף'),\n",
    "    (hfletter.format('\\u05E6'), 'ץ'),\n",
    ")\n",
    "final_heb = [(regex.compile(patt), repl) for patt,repl in final_heb]\n",
    "\n",
    "def sub_final_hb(string):\n",
    "    \"\"\"Substitute final letters in Hebrew\"\"\"\n",
    "    for patt, repl in final_heb:\n",
    "        string = patt.sub(repl, string)\n",
    "    return string\n",
    "\n",
    "def utf8_hebrew(string):\n",
    "    \"\"\"Convert transcribed Hebrew to UTF8\n",
    "    \n",
    "    NB: does not provide final letters (e.g. ם).\n",
    "    \"\"\"\n",
    "    utf8_string = ''\n",
    "    for c in string:\n",
    "        utf8_string += trans2utf8.get(c, '')\n",
    "    utf8_string = sub_final_hb(utf8_string)\n",
    "    return utf8_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'συναγωγὴν'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta2unicode.convert('SUNAGWGH\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'היתה שׁם בבית־לחמם'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_hebrew('HYTH $M B\\BYT-LXMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the files\n",
    "\n",
    "We process the CATSS database files into JSONs. \n",
    "\n",
    "The datastructure is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: hypothetical data for illustration purposes**\n",
    "\n",
    "```\n",
    "[ # list of verses\n",
    "    \n",
    "    [ # list data for a verse\n",
    "    \n",
    "        'Gen 1:9',     # verse reference\n",
    "        \n",
    "        [ # three-list of text columns\n",
    "            \n",
    "            [ # Hebrew column A\n",
    "                ('מקום', {'retcon'}), # text entry + text critical notes\n",
    "            ],\n",
    "            \n",
    "            [ # Hebrew column B\n",
    "                ('מקוה', {'?'}), # text entry + text critical notes\n",
    "            ], \n",
    "            \n",
    "            [ # Greek column\n",
    "                ('συναγωγὴν', {''}), # text entry + text critical notes\n",
    "            ],   \n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some captured contexts may be empty. In this case, the notes are added to empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list consists of a single verse, headed by a reference string. The \n",
    "second element in the list is a three-list of two-tuples.\n",
    "\n",
    "Each two-tuple represents a column in the database, and they consist \n",
    "of `(text, text-critical notes)`. Hebrew column B contains retroverted\n",
    "readings and it is frequently empty.\n",
    "\n",
    "Each column can contain multiple text entries, for cases where there are \n",
    "separate notes per column. For instance, the database might contain \n",
    "words wrapped in curly brackets `{}` with notations that are separate\n",
    "from another word that is not contained in them. Thus, each column\n",
    "can contain more than 1 entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Strategy\n",
    "\n",
    "Each line contains 2-3 columns of data. Those columns contain original language text and markup.\n",
    "\n",
    "We will draw a strong distinction between text and markup, and seek to separate the two. \n",
    "\n",
    "To do this, we first run a pattern search for markup patterns. The markup patterns divide into\n",
    "several subsets, depending on their behavior:\n",
    "\n",
    "* substitutions - represents text elements that have been transported elsewhere (contains no text elements)\n",
    "* context-based - markup that applies within a context of a stretch of text\n",
    "* capturers - markup that captures text within its vicinity based on capture groups\n",
    "\n",
    "If all markup patterns fail to match a string, the algorithm will then run the regex patterns for matching \n",
    "original language text to recognize it as text.\n",
    "\n",
    "A \"context\" is an important concept for the parser. The highest level context is a given column\n",
    "(for instance, Hebrew column A). Contexts can then be split into smaller pieces based on markup\n",
    "capture groups. For instance, the following markup: `{...MQVH <1:23>}` captures everything in\n",
    "between the brackets. Everything in the brackets then becomes a new, smaller context. The pattern\n",
    "matches are run again, recursively, against this context to recognize the text and the additional\n",
    "markup. That markup, if it is of the context-based type, will then only be applied to this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the patterns for matching\n",
    "\n",
    "# original language text\n",
    "hchars = repatts.hchars\n",
    "gchars = repatts.gchars\n",
    "hb_patt = regex.compile(f' *[{hchars}]+ *')\n",
    "grk_patt = regex.compile(f' *[{gchars}]+ *')\n",
    "\n",
    "# markup text\n",
    "comp_patt = lambda pattern: (regex.compile(pattern[0]),) + pattern[1:]\n",
    "common_tc_patts = [comp_patt(p) for p in repatts.common_tc]\n",
    "hb_tc_patts = common_tc_patts + [comp_patt(p) for p in repatts.heb_tc]\n",
    "gk_tc_patts = common_tc_patts + [comp_patt(p) for p in repatts.greek_tc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_element(element):\n",
    "    return element.strip()\n",
    "\n",
    "def parse_context(context, markup_patts, text_patt, position=0, column_list=[], markups=set(), ident='', debug=[]):\n",
    "    \"\"\"Parse a context of text and markup in structured JSON.\"\"\"\n",
    "    \n",
    "    elements = []\n",
    "    \n",
    "    def report(*messages):\n",
    "        debug.extend(ident+m for m in messages)\n",
    "    \n",
    "    report(f'analyzing context: {context}')\n",
    "    \n",
    "    while context and (position < len(context)):\n",
    "    \n",
    "        matched = False # track matches in the loop\n",
    "        \n",
    "        for patt, kind, tag, desc, indices in markup_patts:\n",
    "            \n",
    "            # process markup\n",
    "            if match := patt.match(context, position):\n",
    "                    \n",
    "                # run any optional formats on tag\n",
    "                tag = tag.format(**{k:(match.groups()[i] or '') for k, i in indices.items()})\n",
    "                    \n",
    "                # tag markup within the context\n",
    "                if kind == 'con':\n",
    "                    report(f'  markup pattern match @ {position}: {patt.pattern}', f'    match: {match.group(0)}')\n",
    "                    markups.add(tag)\n",
    "\n",
    "                # run parser recursively for captured sub-contexts\n",
    "                elif kind == 'cap':\n",
    "                    report(f'  markup pattern match @ {position}: {patt.pattern}')\n",
    "                    subcontext = match.groups()[indices['txt']]\n",
    "                    elements.extend(\n",
    "                            parse_context(\n",
    "                                subcontext, markup_patts, text_patt, markups={tag}, column_list=[],\n",
    "                                ident=ident+'    ', debug=debug,\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "                # deal with substitution markups\n",
    "                elif kind == 'sub':\n",
    "                    elements.append(('', {tag}))\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception(f'PATTERN ERROR for {patt}: NO KIND')\n",
    "\n",
    "                # advance the position\n",
    "                position = match.end()\n",
    "                matched = True\n",
    "                break\n",
    "                \n",
    "        if not matched:\n",
    "\n",
    "            # process original language text\n",
    "            if match := text_patt.match(context, position):\n",
    "                elements.append((match.group(), set()))\n",
    "                report(f'\\ttext match @ {position}: {match.group(0)}')\n",
    "                position = match.end()\n",
    "\n",
    "            # no match found, raise a syntax error\n",
    "            elif position < len(context):\n",
    "                error = f'SYNTAX ERROR AT POSITION {position} `{context[position]}` i.e. `{context[position-1:position+2]}`|{context}'\n",
    "                raise Exception(error)\n",
    "      \n",
    "    # we're done\n",
    "    # apply contextual markup to all elements\n",
    "    # and return the goods\n",
    "    if elements:\n",
    "        for element, markup_set in elements:\n",
    "            markup_set |= markups\n",
    "            column_list.append((normalize_element(element), markup_set))\n",
    "    elif markups:\n",
    "        column_list.append(('', markups))\n",
    "        \n",
    "    # recursion depth limit\n",
    "    if len(column_list) >= 100:\n",
    "        error = 'RECURSION DEPTH LIMIT REACHED!'\n",
    "        report('\\n'.join('\\t'+c for c in column_list))\n",
    "        raise Exception(error)\n",
    "        \n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('K/DMWT/NW', {'.doub'}), ('MLK', {'doub.', 'prep', 'trans'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context = \"K/DMWT/NW {d} {...MLK %p}\"\n",
    "debug = []\n",
    "parse_context(test_context, hb_tc_patts, hb_patt, column_list=[], debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing context: K/DMWT/NW {d} {...MLK %p}\n",
      "  markup pattern match @ 0: ({.*?}|[^\\s]+) *(?=\\??\\{d\\})\n",
      "    analyzing context: K/DMWT/NW\n",
      "    \ttext match @ 0: K/DMWT/NW\n",
      "  markup pattern match @ 10: =?\\{d\\}\\??\\s*({.*?}|[^\\s]*) *\n",
      "    analyzing context: {...MLK %p}\n",
      "      markup pattern match @ 0: {\\.\\.\\.(.+?)} *\n",
      "        analyzing context: MLK %p\n",
      "        \ttext match @ 0: MLK \n",
      "          markup pattern match @ 4: =?%p([-+])? *\n",
      "            match: %p\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(debug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning analysis of books\n",
      "\n",
      "parsing 01.Genesis.par...\n",
      "\tbook successfully parsed.\n",
      "DONE\n",
      "\tn-parsed: 20421\n",
      "\tn-errors: 70\n"
     ]
    }
   ],
   "source": [
    "# -- regex patterns --\n",
    "continued_column = regex.compile(r'[^\\s]+.*#\\s*$') # '#' at end of col preceded by some non-space char\n",
    "content = regex.compile(r'.*[^\\s].*') # string has some non-space char (content)\n",
    "\n",
    "def line_is_continued(col1, col2):\n",
    "    \"\"\"Return boolean whether any column in a line is continued in next line\"\"\"\n",
    "    if continued_column.match(col1) or continued_column.match(col2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_dataline(line):\n",
    "    \"\"\"Return boolean on whether a line contains data content\"\"\"\n",
    "    return all([\n",
    "        content.match(line), \n",
    "        not repatts.ref_string.match(line)\n",
    "    ])\n",
    "\n",
    "def get_continued_columns(lines, counter):\n",
    "    \"\"\"Recursively retrieve data-lines continued on next line (marked with #).\n",
    "    \n",
    "    The function recursively retrieves subsequent lines if a starting line\n",
    "    is marked with a continuation marker (#). Each line that is retrieved\n",
    "    must be split into its columns, and those columns in turn must be \n",
    "    checked for continuation markers. This is done recursively until there\n",
    "    is no continuation marker found. The function retrieves the lines using \n",
    "    the current index position; it advances the index by adding 1 each time. \n",
    "    It yields all additional columns it finds as 2-tuples.\n",
    "    \"\"\"\n",
    "    line = lines[counter]\n",
    "    if is_dataline(line):\n",
    "        heb_col, grk_col = line.split('\\t')\n",
    "        if line_is_continued(heb_col, grk_col):\n",
    "            counter += 1\n",
    "            next_cols = lines[counter].split('\\t')\n",
    "            yield next_cols\n",
    "            yield from get_continued_columns(lines, counter) # recursive call here\n",
    "\n",
    "def transcribe_hebrew(string):\n",
    "    \"\"\"Transcribe a string of Hebrew column text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "def transcribe_greek(string):\n",
    "    \"\"\"Transcribe a string of Greek column text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "def clean_text(string, lang='hebrew'):\n",
    "    \"\"\"Clean string of text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "# finalized parallel data goes here\n",
    "para_data = []\n",
    "parsed = collections.defaultdict(list)\n",
    "errors = []\n",
    "debugs = []\n",
    "\n",
    "# process files\n",
    "print('beginning analysis of books\\n')\n",
    "for file in sorted(data.glob('*.par')):\n",
    "    \n",
    "    if file.name == '02.Exodus.par':\n",
    "        print('DONE')\n",
    "        print(f'\\tn-parsed: {len(parsed)}')\n",
    "        print(f'\\tn-errors: {len(errors)}')\n",
    "        break\n",
    "    \n",
    "    print(f'parsing {file.name}...')\n",
    "    \n",
    "    # read the file\n",
    "    lines = file.read_text().split('\\n')\n",
    "    \n",
    "    book_data = [file.stem]\n",
    "    verse_data = []\n",
    "    position = 0\n",
    "    \n",
    "    while position < len(lines):\n",
    "    \n",
    "        line = lines[position]\n",
    "    \n",
    "        # detect a new verse at verse reference string\n",
    "        if repatts.ref_string.match(line):\n",
    "            \n",
    "            # store last verse, make space for new one, store new one\n",
    "            if verse_data:\n",
    "                book_data.append(verse_data)\n",
    "                verse_data = []\n",
    "            verse_data.append(line)\n",
    "        \n",
    "        elif line:\n",
    "            \n",
    "            # for debugging\n",
    "            show_tuple = (file.name, position, verse_data[0], line)\n",
    "            \n",
    "            # extract the two columns\n",
    "            heb_col, grk_col = line.split('\\t')\n",
    "            \n",
    "            # NB: that for Sirach the Hebrew columns can sometimes\n",
    "            # be split several ways since there are numerous Hebrew \n",
    "            # sources, deriving from various manuscripts\n",
    "            # the sources are indicated by a following number;\n",
    "            # thus, it may be possible to split along stand-alone integers\n",
    "            # to divide up the text\n",
    "            \n",
    "            # seperate heb col a and b (optional)\n",
    "            if '=' in heb_col:\n",
    "                heb_colA, heb_colB = heb_col.split('=', 1)\n",
    "            else:\n",
    "                heb_colA = heb_col\n",
    "                heb_colB = ''\n",
    "            \n",
    "            # collect parts of the columns continued on next line(s) in doc\n",
    "            # this is done recursively to ensure all lines are retrieved\n",
    "            cont_cols = list(get_continued_columns(lines, position))\n",
    "            for hb_cc, gk_cc in cont_cols:\n",
    "                position += 1\n",
    "                heb_col += hb_cc\n",
    "                grk_col += gk_cc\n",
    " \n",
    "            # columns are now ready for the parser\n",
    "            # feed into the parser, and if there is a problem\n",
    "            # record it and move on\n",
    "            grammars = [\n",
    "                (heb_colA, hb_tc_patts, hb_patt), \n",
    "                (heb_colB, hb_tc_patts, hb_patt),\n",
    "                (grk_col, gk_tc_patts, grk_patt),\n",
    "            ]\n",
    "            \n",
    "            good = True\n",
    "            debug = [file.name, str(position), f'line: {line}', '-'*30]\n",
    "            column_parsings = []\n",
    "            for context, markup_patts, text_patt in grammars:\n",
    "                try:\n",
    "                    this_parse = parse_context(\n",
    "                        context, \n",
    "                        markup_patts,\n",
    "                        text_patt,\n",
    "                        debug=debug,\n",
    "                        column_list=[],\n",
    "                        markups=set(),\n",
    "                    )\n",
    "                    column_parsings.append(this_parse)\n",
    "                except:\n",
    "                    einfo = ' '.join(str(e) for e in list(sys.exc_info())[:2])\n",
    "                    debug.append(einfo)\n",
    "                    errors.append(debug)\n",
    "                    good = False\n",
    "                    break\n",
    "              \n",
    "            if good:\n",
    "                parsed[f'{file.name}.{position}'] = column_parsings\n",
    "                debugs.append(debug)\n",
    "                verse_data.extend(column_parsings)\n",
    "              \n",
    "        # it's an empty line; move on\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        position += 1\n",
    "        \n",
    "    para_data.append(book_data)\n",
    "    print(f'\\tbook successfully parsed.')\n",
    "    \n",
    "def show_errors(errors):\n",
    "    for error in errors:\n",
    "        print('\\n'.join(error))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<regex.Match object; span=(0, 2), match='? '>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex Testing Cell\n",
    "\n",
    "test_re = regex.compile(\"(^| +)\\?+( +|$)\")\n",
    "test_str = \"? ---\"\n",
    "position = 0\n",
    "match = test_re.match(test_str, position)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('W/L/$MR/H', set())],\n",
       " [],\n",
       " [('KAI\\\\', set()), ('FULA/SSEIN', set()), ('AU)TO\\\\N', {'distr'})]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed['01.Genesis.par.719']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.Genesis.par\n",
      "1442\n",
      "line: N( =@?)NX .(x\tSTE/NWN\n",
      "------------------------------\n",
      "analyzing context: N( \n",
      "\ttext match @ 0: N( \n",
      "analyzing context: @?)NX .(x\n",
      "  markup pattern match @ 0: =?@([)BGDHWZX+YKLMNS(PCQR&$T/ ?,]*)\n",
      "    analyzing context: ?)NX \n",
      "      markup pattern match @ 0: (^| +)\\?+([^\\s?]+ *)\n",
      "        analyzing context: )NX \n",
      "        \ttext match @ 0: )NX \n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 6 `.` i.e. ` .(`|@?)NX .(x\n",
      "\n",
      "01.Genesis.par\n",
      "1466\n",
      "line: N( =@?)NX .(x\tSTE/NWN\n",
      "------------------------------\n",
      "analyzing context: N( \n",
      "\ttext match @ 0: N( \n",
      "analyzing context: @?)NX .(x\n",
      "  markup pattern match @ 0: =?@([)BGDHWZX+YKLMNS(PCQR&$T/ ?,]*)\n",
      "    analyzing context: ?)NX \n",
      "      markup pattern match @ 0: (^| +)\\?+([^\\s?]+ *)\n",
      "        analyzing context: )NX \n",
      "        \ttext match @ 0: )NX \n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 6 `.` i.e. ` .(`|@?)NX .(x\n",
      "\n",
      "01.Genesis.par\n",
      "1578\n",
      "line: --+ =?HW) HYH <4.20>\t{...KAI\\ H)=N}?\n",
      "------------------------------\n",
      "analyzing context: --+ \n",
      "  markup pattern match @ 0: --\\+\\s?(''|{x})? *\n",
      "    match: --+ \n",
      "analyzing context: ?HW) HYH <4.20>\n",
      "  markup pattern match @ 0: (^| +)\\?+([^\\s?]+ *)\n",
      "    analyzing context: HW) \n",
      "    \ttext match @ 0: HW) \n",
      "\ttext match @ 5: HYH \n",
      "  markup pattern match @ 9: <(.*?)> *\n",
      "    match: <4.20>\n",
      "analyzing context: {...KAI\\ H)=N}?\n",
      "  markup pattern match @ 0: {\\.\\.\\.(.+?)} *\n",
      "    analyzing context: KAI\\ H)=N\n",
      "    \ttext match @ 0: KAI\\ \n",
      "    \ttext match @ 5: H)=N\n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 14 `?` i.e. `}?`|{...KAI\\ H)=N}?\n",
      "\n",
      "01.Genesis.par\n",
      "1951\n",
      "line: {...$TYM W/$MWNYM} W/$B( #\tO)KTAKO/SIA ^ DU/O\n",
      "------------------------------\n",
      "analyzing context: {...$TYM W/$MWNYM} W/$B( #\n",
      "  markup pattern match @ 0: {\\.\\.\\.(.+?)} *\n",
      "    analyzing context: $TYM W/$MWNYM\n",
      "    \ttext match @ 0: $TYM \n",
      "    \ttext match @ 5: W/$MWNYM\n",
      "\ttext match @ 19: W/$B( \n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 25 `#` i.e. ` #`|{...$TYM W/$MWNYM} W/$B( #\n",
      "\n",
      "01.Genesis.par\n",
      "2019\n",
      "line: {...$B( W/$B(YM} W/$B( M)WT #\tE(PTAKO/SIA ^ KAI\\ PENTH/KONTA TRI/A\n",
      "------------------------------\n",
      "analyzing context: {...$B( W/$B(YM} W/$B( M)WT #\n",
      "  markup pattern match @ 0: {\\.\\.\\.(.+?)} *\n",
      "    analyzing context: $B( W/$B(YM\n",
      "    \ttext match @ 0: $B( \n",
      "    \ttext match @ 4: W/$B(YM\n",
      "\ttext match @ 17: W/$B( \n",
      "\ttext match @ 23: M)WT \n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 28 `#` i.e. ` #`|{...$B( W/$B(YM} W/$B( M)WT #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_errors(errors[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Line-merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a prototype for collecting the next lines recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Path('../source/patched/17.1Esdras.par').read_text().split('\\n')\n",
    "\n",
    "# lines = test[3067]\n",
    "\n",
    "# lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i = 0\n",
    "# while i < len(lines):\n",
    "    \n",
    "#     line = lines[i]\n",
    "    \n",
    "#     if not line or ref_string.match(line):\n",
    "#         i += 1\n",
    "#         continue\n",
    "    \n",
    "#     print(line)\n",
    "#     heb_col, grk_col = line.split('\\t')\n",
    "#     cont_lines = list(get_continued_columns(lines, i))\n",
    "#     #print(cont_lines)\n",
    "#     for hb_cc, gk_cc in cont_lines:\n",
    "#         i += 1 # advance position in doc\n",
    "#         heb_col += hb_cc\n",
    "#         grk_col += gk_cc\n",
    "\n",
    "#     print('\\theb:', heb_col)\n",
    "#     print('\\tgrk:', grk_col)\n",
    "#     print()\n",
    "            \n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_column = re.compile(r'[^\\s]+.*#\\s*$')\n",
    "\n",
    "# for line in lines:\n",
    "#     for col in line.split('\\t'):\n",
    "#         if cont_column.match(col):\n",
    "#             print(col, 'match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_is_continued(*'BR)Y W/B/$(RYM =:BR)WM$(RYM #\\tBAROUMSEWRIM {t}'.split('\\t'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
