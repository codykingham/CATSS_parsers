{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Parallel Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean dataset is a modernization of the CATSS Database, presented with minimal changes, in UTF8, exported as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import regex \n",
    "import collections\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from greekutils import beta2unicode # do: pip install greek-utils==0.2\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append('../')\n",
    "import regex_patterns as repatts\n",
    "\n",
    "data = Path('../source/patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCAT transcription to UTF8\n",
    "# Greek to be handled by greekutils\n",
    "\n",
    "# Hebrew\n",
    "trans2utf8 = {\n",
    "    ')': 'א',\n",
    "    'B': 'ב',\n",
    "    'G': 'ג',\n",
    "    'D': 'ד',\n",
    "    'H': 'ה',\n",
    "    'W': 'ו',\n",
    "    'Z': 'ז',\n",
    "    'X': 'ח',\n",
    "    '+': 'ט',\n",
    "    'Y': 'י',\n",
    "    'K': 'כ',\n",
    "    'L': 'ל',\n",
    "    'M': 'מ',\n",
    "    'N': 'נ',\n",
    "    'S': 'ס',\n",
    "    '(': 'ע',\n",
    "    'P': 'פ',\n",
    "    'C': 'צ',\n",
    "    'Q': 'ק',\n",
    "    'R': 'ר',\n",
    "    '&': 'שׂ',\n",
    "    '$': 'שׁ',\n",
    "    'T': 'ת',\n",
    "    '-': '־',\n",
    "    '\\\\': '',\n",
    "    ' ': ' ',\n",
    "}\n",
    "\n",
    "hfletter = r'{}(?=\\s|$)'\n",
    "final_heb = (\n",
    "    (hfletter.format('\\u05DE'), 'ם'),\n",
    "    (hfletter.format('\\u05DB'), 'ך'),\n",
    "    (hfletter.format('\\u05E0'), 'ן'),\n",
    "    (hfletter.format('\\u05E4'), 'ף'),\n",
    "    (hfletter.format('\\u05E6'), 'ץ'),\n",
    ")\n",
    "final_heb = [(regex.compile(patt), repl) for patt,repl in final_heb]\n",
    "\n",
    "def sub_final_hb(string):\n",
    "    \"\"\"Substitute final letters in Hebrew\"\"\"\n",
    "    for patt, repl in final_heb:\n",
    "        string = patt.sub(repl, string)\n",
    "    return string\n",
    "\n",
    "def utf8_hebrew(string):\n",
    "    \"\"\"Convert transcribed Hebrew to UTF8\n",
    "    \n",
    "    NB: does not provide final letters (e.g. ם).\n",
    "    \"\"\"\n",
    "    utf8_string = ''\n",
    "    for c in string:\n",
    "        utf8_string += trans2utf8.get(c, '')\n",
    "    utf8_string = sub_final_hb(utf8_string)\n",
    "    return utf8_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'συναγωγὴν'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta2unicode.convert('SUNAGWGH\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'היתה שׁם בבית־לחמם'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_hebrew('HYTH $M B\\BYT-LXMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the files\n",
    "\n",
    "We process the CATSS database files into JSONs. \n",
    "\n",
    "The datastructure is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: hypothetical data for illustration purposes**\n",
    "\n",
    "```\n",
    "[ # list of verses\n",
    "    \n",
    "    [ # list data for a verse\n",
    "    \n",
    "        'Gen 1:9',     # verse reference\n",
    "        \n",
    "        [ # three-list of text columns\n",
    "            \n",
    "            [ # Hebrew column A\n",
    "                ('מקום', {'retcon'}), # text entry + text critical notes\n",
    "            ],\n",
    "            \n",
    "            [ # Hebrew column B\n",
    "                ('מקוה', {'?'}), # text entry + text critical notes\n",
    "            ], \n",
    "            \n",
    "            [ # Greek column\n",
    "                ('συναγωγὴν', {''}), # text entry + text critical notes\n",
    "            ],   \n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some captured contexts may be empty. In this case, the notes are added to empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each list consists of a single verse, headed by a reference string. The \n",
    "second element in the list is a three-list of two-tuples.\n",
    "\n",
    "Each two-tuple represents a column in the database, and they consist \n",
    "of `(text, text-critical notes)`. Hebrew column B contains retroverted\n",
    "readings and it is frequently empty.\n",
    "\n",
    "Each column can contain multiple text entries, for cases where there are \n",
    "separate notes per column. For instance, the database might contain \n",
    "words wrapped in curly brackets `{}` with notations that are separate\n",
    "from another word that is not contained in them. Thus, each column\n",
    "can contain more than 1 entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Strategy\n",
    "\n",
    "Each line contains 2-3 columns of data. Those columns contain original language text and markup.\n",
    "\n",
    "We will draw a strong distinction between text and markup, and seek to separate the two. \n",
    "\n",
    "To do this, we first run a pattern search for markup patterns. The markup patterns divide into\n",
    "several subsets, depending on their behavior:\n",
    "\n",
    "* substitutions - represents text elements that have been transported elsewhere (contains no text elements)\n",
    "* context-based - markup that applies within a context of a stretch of text\n",
    "* capturers - markup that captures text within its vicinity based on capture groups\n",
    "\n",
    "If all markup patterns fail to match a string, the algorithm will then run the regex patterns for matching \n",
    "original language text to recognize it as text.\n",
    "\n",
    "A \"context\" is an important concept for the parser. The highest level context is a given column\n",
    "(for instance, Hebrew column A). Contexts can then be split into smaller pieces based on markup\n",
    "capture groups. For instance, the following markup: `{...MQVH <1:23>}` captures everything in\n",
    "between the brackets. Everything in the brackets then becomes a new, smaller context. The pattern\n",
    "matches are run again, recursively, against this context to recognize the text and the additional\n",
    "markup. That markup, if it is of the context-based type, will then only be applied to this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the patterns for matching\n",
    "\n",
    "# original language text\n",
    "hchars = repatts.hchars\n",
    "gchars = repatts.gchars\n",
    "hb_patt = regex.compile(f' *[{hchars}]+ *')\n",
    "grk_patt = regex.compile(f' *[{gchars}]+ *')\n",
    "discard = regex.compile(repatts.discard)\n",
    "\n",
    "# markup text\n",
    "comp_patt = lambda pattern: (regex.compile(pattern[0]),) + pattern[1:]\n",
    "common_tc_patts = [comp_patt(p) for p in repatts.common_tc]\n",
    "hb_tc_patts = common_tc_patts + [comp_patt(p) for p in repatts.heb_tc]\n",
    "gk_tc_patts = common_tc_patts + [comp_patt(p) for p in repatts.greek_tc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_element(element):\n",
    "    return element.strip()\n",
    "\n",
    "def parse_context(context, markup_patts, text_patt, position=0, column_list=[], markups=set(), ident='', debug=[]):\n",
    "    \"\"\"Parse a context of text and markup in structured JSON.\"\"\"\n",
    "    \n",
    "    elements = []\n",
    "    \n",
    "    def report(*messages):\n",
    "        debug.extend(ident+m for m in messages)\n",
    "    \n",
    "    report(f'analyzing context: `{context}`')\n",
    "    \n",
    "    while context and (position < len(context)):\n",
    "    \n",
    "        matched = False # track matches in the loop\n",
    "        \n",
    "        for patt, kind, tag, desc, indices in markup_patts:\n",
    "            \n",
    "            # process markup\n",
    "            if match := patt.match(context, position):\n",
    "                    \n",
    "                # run any optional formats on tag\n",
    "                tag = tag.format(**{k:(match.groups()[i] or '') for k, i in indices.items()})\n",
    "                    \n",
    "                # tag markup within the context\n",
    "                if kind == 'con':\n",
    "                    report(f'  markup pattern match @ {position}: {patt.pattern}', f'    match: `{match.group(0)}`')\n",
    "                    markups.add(tag)\n",
    "\n",
    "                # run parser recursively for captured sub-contexts\n",
    "                elif kind == 'cap':\n",
    "                    report(f'  markup pattern match @ {position}: {patt.pattern}')\n",
    "                    subcontext = match.groups()[indices['txt']]\n",
    "                    elements.extend(\n",
    "                            parse_context(\n",
    "                                subcontext, markup_patts, text_patt, markups={tag}, column_list=[],\n",
    "                                ident=ident+'    ', debug=debug,\n",
    "                            )\n",
    "                    )\n",
    "\n",
    "                # deal with substitution markups\n",
    "                elif kind == 'sub':\n",
    "                    elements.append(('', {tag}))\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception(f'PATTERN ERROR for {patt}: NO KIND')\n",
    "\n",
    "                # advance the position\n",
    "                position = match.end()\n",
    "                matched = True\n",
    "                break\n",
    "                \n",
    "        if not matched:\n",
    "\n",
    "            # process original language text\n",
    "            if match := text_patt.match(context, position):\n",
    "                elements.append((match.group(), set()))\n",
    "                report(f'  text match @ {position}: `{match.group(0)}`')\n",
    "                position = match.end()\n",
    "\n",
    "            # process discard strings\n",
    "            elif match := discard.match(context, position):\n",
    "                report(f'  discarding string @ {position}: `{match.group(0)}`')\n",
    "                position = match.end()\n",
    "                \n",
    "            # no match found, raise a syntax error\n",
    "            elif position < len(context):\n",
    "                error = f'SYNTAX ERROR AT POSITION {position} `{context[position]}` i.e. `{context[position-1:position+2]}` in `{context}`'\n",
    "                raise Exception(error)\n",
    "      \n",
    "    # we're done\n",
    "    # apply contextual markup to all elements\n",
    "    # and return the goods\n",
    "    if elements:\n",
    "        for element, markup_set in elements:\n",
    "            markup_set |= markups\n",
    "            column_list.append((normalize_element(element), markup_set))\n",
    "    elif markups:\n",
    "        column_list.append(('', markups))\n",
    "        \n",
    "    # recursion depth limit\n",
    "    if len(column_list) >= 100:\n",
    "        error = 'RECURSION DEPTH LIMIT REACHED!'\n",
    "        report('\\n'.join('\\t'+c for c in column_list))\n",
    "        raise Exception(error)\n",
    "        \n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('K/DMWT/NW', {'.doub'}), ('MLK', {'doub.', 'prep', 'trans'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_context = \"K/DMWT/NW {d} {...MLK %p}\"\n",
    "debug = []\n",
    "parse_context(test_context, hb_tc_patts, hb_patt, column_list=[], debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing context: `K/DMWT/NW {d} {...MLK %p}`\n",
      "  markup pattern match @ 0: ({.*?}|[^\\s]+) *(?=\\??\\{d\\})\n",
      "    analyzing context: `K/DMWT/NW`\n",
      "      text match @ 0: `K/DMWT/NW`\n",
      "  markup pattern match @ 10: =?\\{d\\}\\??\\s*({.*?}|[^\\s]*)\n",
      "    analyzing context: `{...MLK %p}`\n",
      "      markup pattern match @ 0: {\\.\\.\\.(.+?)}\n",
      "        analyzing context: `MLK %p`\n",
      "          text match @ 0: `MLK `\n",
      "          markup pattern match @ 4: =?%p([-+])?\n",
      "            match: `%p`\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(debug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning analysis of books\n",
      "\n",
      "parsing 01.Genesis.par...\n",
      "\tbook parsed.\n",
      "parsing 02.Exodus.par...\n",
      "\tbook parsed.\n",
      "parsing 03.Lev.par...\n",
      "\tbook parsed.\n",
      "parsing 04.Num.par...\n",
      "\tbook parsed.\n",
      "parsing 05.Deut.par...\n",
      "\tbook parsed.\n",
      "parsing 06.JoshB.par...\n",
      "\tbook parsed.\n",
      "parsing 07.JoshA.par...\n",
      "\tbook parsed.\n",
      "parsing 08.JudgesB.par...\n",
      "\tbook parsed.\n",
      "parsing 09.JudgesA.par...\n",
      "\tbook parsed.\n",
      "parsing 10.Ruth.par...\n",
      "\tbook parsed.\n",
      "parsing 11.1Sam.par...\n",
      "\tbook parsed.\n",
      "parsing 12.2Sam.par...\n",
      "\tbook parsed.\n",
      "parsing 13.1Kings.par...\n",
      "\tbook parsed.\n",
      "parsing 14.2Kings.par...\n",
      "\tbook parsed.\n",
      "parsing 15.1Chron.par...\n",
      "\tbook parsed.\n",
      "parsing 16.2Chron.par...\n",
      "\tbook parsed.\n",
      "skipping 17.1Esdras.par\n",
      "parsing 18.Esther.par...\n",
      "\tbook parsed.\n",
      "parsing 18.Ezra.par...\n",
      "\tbook parsed.\n",
      "parsing 19.Neh.par...\n",
      "\tbook parsed.\n",
      "parsing 20.Psalms.par...\n",
      "\tbook parsed.\n",
      "skipping 22.Ps151.par\n",
      "parsing 23.Prov.par...\n",
      "\tbook parsed.\n",
      "parsing 24.Qoh.par...\n",
      "\tbook parsed.\n",
      "parsing 25.Cant.par...\n",
      "\tbook parsed.\n",
      "parsing 26.Job.par...\n",
      "\tbook parsed.\n",
      "skipping 27.Sirach.par\n",
      "parsing 28.Hosea.par...\n",
      "\tbook parsed.\n",
      "parsing 29.Micah.par...\n",
      "\tbook parsed.\n",
      "parsing 30.Amos.par...\n",
      "\tbook parsed.\n",
      "parsing 31.Joel.par...\n",
      "\tbook parsed.\n",
      "parsing 32.Jonah.par...\n",
      "\tbook parsed.\n",
      "parsing 33.Obadiah.par...\n",
      "\tbook parsed.\n",
      "parsing 34.Nahum.par...\n",
      "\tbook parsed.\n",
      "parsing 35.Hab.par...\n",
      "\tbook parsed.\n",
      "parsing 36.Zeph.par...\n",
      "\tbook parsed.\n",
      "parsing 37.Haggai.par...\n",
      "\tbook parsed.\n",
      "parsing 38.Zech.par...\n",
      "\tbook parsed.\n",
      "parsing 39.Malachi.par...\n",
      "\tbook parsed.\n",
      "parsing 40.Isaiah.par...\n",
      "\tbook parsed.\n",
      "parsing 41.Jer.par...\n",
      "\tbook parsed.\n",
      "parsing 42.Baruch.par...\n",
      "\tbook parsed.\n",
      "parsing 43.Lam.par...\n",
      "\tbook parsed.\n",
      "parsing 44.Ezekiel.par...\n",
      "\tbook parsed.\n",
      "parsing 45.DanielOG.par...\n",
      "\tbook parsed.\n",
      "parsing 46.DanielTh.par...\n",
      "\tbook parsed.\n",
      "DONE\n",
      "\tn-parsed: 329388\n",
      "\tn-errors: 91\n",
      "Errors by book:\n",
      "\t08.JudgesB.par - 1\n",
      "\t09.JudgesA.par - 1\n",
      "\t10.Ruth.par - 3\n",
      "\t11.1Sam.par - 1\n",
      "\t12.2Sam.par - 11\n",
      "\t13.1Kings.par - 5\n",
      "\t14.2Kings.par - 7\n",
      "\t18.Esther.par - 2\n",
      "\t18.Ezra.par - 2\n",
      "\t20.Psalms.par - 2\n",
      "\t23.Prov.par - 11\n",
      "\t26.Job.par - 6\n",
      "\t35.Hab.par - 1\n",
      "\t36.Zeph.par - 1\n",
      "\t40.Isaiah.par - 11\n",
      "\t41.Jer.par - 11\n",
      "\t44.Ezekiel.par - 5\n",
      "\t45.DanielOG.par - 10\n"
     ]
    }
   ],
   "source": [
    "# -- regex patterns --\n",
    "continued_column = regex.compile(r'[^\\s]+.*#\\s*$') # '#' at end of col preceded by some non-space char\n",
    "content = regex.compile(r'.*[^\\s].*') # string has some non-space char (content)\n",
    "\n",
    "def line_is_continued(col1, col2):\n",
    "    \"\"\"Return boolean whether any column in a line is continued in next line\"\"\"\n",
    "    if continued_column.match(col1) or continued_column.match(col2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_dataline(line):\n",
    "    \"\"\"Return boolean on whether a line contains data content\"\"\"\n",
    "    return all([\n",
    "        content.match(line), \n",
    "        not repatts.ref_string.match(line)\n",
    "    ])\n",
    "\n",
    "def get_continued_columns(lines, counter):\n",
    "    \"\"\"Recursively retrieve data-lines continued on next line (marked with #).\n",
    "    \n",
    "    The function recursively retrieves subsequent lines if a starting line\n",
    "    is marked with a continuation marker (#). Each line that is retrieved\n",
    "    must be split into its columns, and those columns in turn must be \n",
    "    checked for continuation markers. This is done recursively until there\n",
    "    is no continuation marker found. The function retrieves the lines using \n",
    "    the current index position; it advances the index by adding 1 each time. \n",
    "    It yields all additional columns it finds as 2-tuples.\n",
    "    \"\"\"\n",
    "    line = lines[counter]\n",
    "    if is_dataline(line):\n",
    "        heb_col, grk_col = line.split('\\t')\n",
    "        if line_is_continued(heb_col, grk_col):\n",
    "            counter += 1\n",
    "            next_cols = lines[counter].split('\\t')\n",
    "            yield next_cols\n",
    "            yield from get_continued_columns(lines, counter) # recursive call here\n",
    "\n",
    "def transcribe_hebrew(string):\n",
    "    \"\"\"Transcribe a string of Hebrew column text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "def transcribe_greek(string):\n",
    "    \"\"\"Transcribe a string of Greek column text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "def clean_text(string, lang='hebrew'):\n",
    "    \"\"\"Clean string of text from the parallel database.\"\"\"\n",
    "    pass\n",
    "\n",
    "# finalized parallel data goes here\n",
    "para_data = []\n",
    "errors = []\n",
    "parsed = collections.defaultdict(list)\n",
    "debugs = collections.defaultdict(list)\n",
    "book_errors = collections.Counter()\n",
    "\n",
    "non_canon = {'17.1Esdras.par', '22.Ps151.par', '27.Sirach.par'}\n",
    "\n",
    "# process files\n",
    "print('beginning analysis of books\\n')\n",
    "for file in sorted(data.glob('*.par')):\n",
    "    \n",
    "    # stop at errors to allow fixing\n",
    "#     if errors:\n",
    "#         print('!! stopping with errors !! ')\n",
    "#         break\n",
    "    \n",
    "    if file.name in non_canon:\n",
    "        print(f'skipping {file.name}')\n",
    "        continue\n",
    "    \n",
    "    print(f'parsing {file.name}...')\n",
    "    \n",
    "    # read the file\n",
    "    lines = file.read_text().split('\\n')\n",
    "    \n",
    "    book_data = [file.stem]\n",
    "    verse_data = []\n",
    "    position = 0\n",
    "    \n",
    "    while position < len(lines):\n",
    "    \n",
    "        line = lines[position]\n",
    "    \n",
    "        # detect a new verse at verse reference string\n",
    "        if repatts.ref_string.match(line):\n",
    "            \n",
    "            # store last verse, make space for new one, store new one\n",
    "            if verse_data:\n",
    "                book_data.append(verse_data)\n",
    "                verse_data = []\n",
    "            verse_data.append(line)\n",
    "        \n",
    "        elif line:\n",
    "            \n",
    "            # for debugging\n",
    "            show_tuple = (file.name, position, verse_data[0], line)\n",
    "            \n",
    "            # extract the two columns\n",
    "            heb_col, grk_col = line.split('\\t')\n",
    "            \n",
    "            # NB: that for Sirach the Hebrew columns can sometimes\n",
    "            # be split several ways since there are numerous Hebrew \n",
    "            # sources, deriving from various manuscripts\n",
    "            # the sources are indicated by a following number;\n",
    "            # thus, it may be possible to split along stand-alone integers\n",
    "            # to divide up the text\n",
    "            \n",
    "            # collect parts of the columns continued on next line(s) in doc\n",
    "            # this is done recursively to ensure all lines are retrieved\n",
    "            cont_cols = list(get_continued_columns(lines, position))\n",
    "            for hb_cc, gk_cc in cont_cols:\n",
    "                position += 1\n",
    "                heb_col += hb_cc\n",
    "                grk_col += gk_cc\n",
    "\n",
    "            # seperate heb col a and b (optional)\n",
    "            if '=' in heb_col:\n",
    "                heb_colA, heb_colB = heb_col.split('=', 1)\n",
    "            else:\n",
    "                heb_colA = heb_col\n",
    "                heb_colB = ''\n",
    "                \n",
    "            # remove column continuation marker since it's already been handled\n",
    "            heb_colA = heb_colA.replace('#', '')\n",
    "            heb_colB = heb_colB.replace('#', '')\n",
    "            grk_col = grk_col.replace('#', '')\n",
    "                \n",
    "            # columns are now ready for the parser\n",
    "            # feed into the parser, and if there is a problem\n",
    "            # record it and move on\n",
    "            grammars = [\n",
    "                (heb_colA, hb_tc_patts, hb_patt), \n",
    "                (heb_colB, hb_tc_patts, hb_patt),\n",
    "                (grk_col, gk_tc_patts, grk_patt),\n",
    "            ]\n",
    "            \n",
    "            good = True\n",
    "            debug = [file.name, str(position), f'line: {line}', '-'*30]\n",
    "            column_parsings = []\n",
    "            for context, markup_patts, text_patt in grammars:\n",
    "                try:\n",
    "                    this_parse = parse_context(\n",
    "                        context, \n",
    "                        markup_patts,\n",
    "                        text_patt,\n",
    "                        debug=debug,\n",
    "                        column_list=[],\n",
    "                        markups=set(),\n",
    "                    )\n",
    "                    column_parsings.append(this_parse)\n",
    "                except:\n",
    "                    einfo = ' '.join(str(e) for e in list(sys.exc_info())[:2])\n",
    "                    debug.append(einfo)\n",
    "                    errors.append(debug)\n",
    "                    book_errors[file.name] += 1\n",
    "                    good = False\n",
    "                    break\n",
    "              \n",
    "            if good:\n",
    "                case = f'{file.name}.{position}'\n",
    "                parsed[case] = column_parsings\n",
    "                debugs[case] = debug\n",
    "                verse_data.extend(column_parsings)\n",
    "              \n",
    "        # it's an empty line; move on\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        position += 1\n",
    "        \n",
    "    para_data.append(book_data)\n",
    "    \n",
    "    print(f'\\tbook parsed.')\n",
    "    \n",
    "print('DONE')\n",
    "print(f'\\tn-parsed: {len(parsed)}')\n",
    "print(f'\\tn-errors: {len(errors)}')\n",
    "print('Errors by book:')\n",
    "for book, count in book_errors.items():\n",
    "    print(f'\\t{book} - {count}')\n",
    "    \n",
    "def show_errors(errors):\n",
    "    for error in errors:\n",
    "        print('\\n'.join(error))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regex Testing Cell\n",
    "\n",
    "# test_re = regex.compile(fr\"[.^]+(?~/.zshrc[#])\")\n",
    "# test_str = \"{..pEI)S) TO\\\\N DRUMO\\\\N\"\n",
    "# position = 0\n",
    "# match = test_re.match(test_str, position)\n",
    "# match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('BRKT', {'.^'}), ('', {'^.', 'infa.p'})],\n",
       " [('BRK', set())],\n",
       " [('EU)LOGW=N', set())]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed['04.Num.par.12734']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\n'.join(debugs['02.Exodus.par.17987']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08.JudgesB.par\n",
      "9903\n",
      "line: * **BNY {**}\tOI( UI(OI\\\n",
      "------------------------------\n",
      "analyzing context: `* **BNY {**}`\n",
      "<class 'Exception'> SYNTAX ERROR AT POSITION 0 `*` i.e. `` in `* **BNY {**}`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_errors(errors[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for position, parsing in parsed.items():\n",
    "#     print(position)\n",
    "#     print(parsing)\n",
    "#     print()\n",
    "#     i += 1\n",
    "#     if i > 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Line-merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a prototype for collecting the next lines recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--+ =TRWMT,H/TNWPH <30.13, #\\tA)FAI/REMA [39.2]', '38.29^>\\t#']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Path('../source/patched/02.Exodus.par').read_text().split('\\n')\n",
    "\n",
    "lines = test[17986:17988]\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--+ =TRWMT,H/TNWPH <30.13, #\tA)FAI/REMA [39.2]\n",
      "\theb: --+ =TRWMT,H/TNWPH <30.13, #38.29^>\n",
      "\tgrk: A)FAI/REMA [39.2]#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(lines):\n",
    "    \n",
    "    line = lines[i]\n",
    "    \n",
    "    if not line or repatts.ref_string.match(line):\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    print(line)\n",
    "    heb_col, grk_col = line.split('\\t')\n",
    "    cont_lines = list(get_continued_columns(lines, i))\n",
    "    #print(cont_lines)\n",
    "    for hb_cc, gk_cc in cont_lines:\n",
    "        i += 1 # advance position in doc\n",
    "        heb_col += hb_cc\n",
    "        grk_col += gk_cc\n",
    "\n",
    "    print('\\theb:', heb_col)\n",
    "    print('\\tgrk:', grk_col)\n",
    "    print()\n",
    "            \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_column = re.compile(r'[^\\s]+.*#\\s*$')\n",
    "\n",
    "# for line in lines:\n",
    "#     for col in line.split('\\t'):\n",
    "#         if cont_column.match(col):\n",
    "#             print(col, 'match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_is_continued(*'BR)Y W/B/$(RYM =:BR)WM$(RYM #\\tBAROUMSEWRIM {t}'.split('\\t'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
